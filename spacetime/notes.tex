\documentclass[a4paper,12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts,stmaryrd}
\usepackage{soul}
\usepackage{array}
\usepackage{empheq}
\usepackage{xfrac}
\usepackage{minibox}
\usepackage{enumitem}
	\setlist{nosep} % or \setlist{noitemsep} to leave space around whole list
\usepackage{color}

\newcommand{\tcb}{\textcolor{blue}}
\newcommand{\todo}[1]{\textcolor{red}{[TODO\@: #1]}}



\begin{document}
\allowdisplaybreaks


% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\section{Space-time discretizations}\label{sec:disc}

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{Time integrators}

Consider a linear PDE of the form 
\begin{align*}
u_t + \mathcal{L}(u,\mathbf{x},t) = g(\mathbf{x},t).
\end{align*}
We discretize $\mathcal{L}$ in space and, denoting $\mathbf{u}(t)$ the discrete solution in space at time $t$
and $\mathbf{g}(t)$ the corresponding right-hand side, we arrive at the system of ODEs 
\begin{align}\label{eq:ode}
\mathbf{u}_t + \mathcal{L}(t)\mathbf{u} = \mathbf{g}(t).
\end{align}
Time integrators can be largely broken down into Runge-Kutta-type methods and linear multistep methods. Runge-Kutta methods
are one-step, $s$-stage methods, and are not amenable to building a space-time linear system. Three main classes of linear multistep
methods are the Adams-Bashforth (explicit), Adams-Moulton (implicit), and backwards differentiation formulas (implicit), all of which
can be framed as a sparse linear system. Suppose we have a discrete time grid with time step $\Delta t$ and time points $\{t_0,t_1,...\}$.
Denote $\mathbf{g}(t_n) = \mathbf{g}_n$ and $\mathcal{L}(t_n) = \mathcal{L}_n$. The first three formulae for each of the linear
multistep classes of integrators in the context of \eqref{eq:ode} are given as follows:
%
{\small
\begin{align*}
\textnormal{\textbf{Adams-Bashforth:}}\hspace{40.5ex}&\\
\mathbf{u}_n - (I - \Delta t\mathcal{L}_{n-1})\mathbf{u}_{n-1} & =\Delta t\mathbf{g}_{n-1}  \\
\mathbf{u}_n - (I - \tfrac{3}{2}\Delta t\mathcal{L}_{n-1})\mathbf{u}_{n-1} - \tfrac{1}{2}\Delta t\mathcal{L}_{n-2}\mathbf{u}_{n-2} & = \tfrac{3}{2}\Delta t\mathbf{g}_{n-1} - \tfrac{1}{2}\Delta t\mathbf{g}_{n-2}\\
\mathbf{u}_n - (I - \tfrac{23}{12}\Delta t\mathcal{L}_{n-1})\mathbf{u}_{n-1} - \tfrac{4}{3}\Delta t\mathcal{L}_{n-2}\mathbf{u}_{n-2} + 
	\tfrac{5}{12}\Delta t\mathcal{L}_{n-3}\mathbf{u}_{n-3} & = \tfrac{23}{12}\Delta t\mathbf{g}_{n-1} - \tfrac{4}{3}\Delta t\mathbf{g}_{n-2} + 
	\tfrac{5}{12}\Delta t\mathbf{g}_{n-3}\\
\textnormal{\textbf{Adams-Moulton:}}\hspace{42ex}&\\
(I + \Delta t \mathcal{L}_n)\mathbf{u}_n - \mathbf{u}_{n-1} &  =  \Delta t \mathbf{g}_n \\
(I + \tfrac{1}{2}\Delta t \mathcal{L}_n)\mathbf{u}_n - (I - \tfrac{1}{2}\Delta t \mathcal{L}_{n-1})\mathbf{u}_{n-1} & =
	\tfrac{1}{2} \Delta t (\mathbf{g}_n + \mathbf{g}_{n-1}) \\
(I + \tfrac{5}{12}\Delta t \mathcal{L}_n)\mathbf{u}_n - (I - \tfrac{2}{3}\Delta t \mathcal{L}_{n-1})\mathbf{u}_{n-1} - 
	\tfrac{1}{12}\Delta t\mathcal{L}_{n-2}\mathbf{u}_{n-2}& = \tfrac{5}{12} \Delta t\mathbf{g}_n + \tfrac{2}{3} \Delta t\mathbf{g}_{n-1} -
	\tfrac{1}{12} \Delta t\mathbf{g}_{n-2}\\
\textnormal{\textbf{BDF:}}\hspace{55ex}&\\
(I + \Delta t \mathcal{L}_n)\mathbf{u}_n - \mathbf{u}_{n-1} &  =  \Delta t \mathbf{g}_n \\
 (I + \tfrac{2}{3}\Delta t \mathcal{L}_n)\mathbf{u}_{n} - \tfrac{4}{3} \mathbf{u}_{n-1} + \tfrac{1}{3} \mathbf{u}_{n-2} & =
 	\tfrac{2}{3}\Delta t\mathbf{g}_n \\
(I + \tfrac{6}{11}\Delta t \mathcal{L}_n)\mathbf{u}_{n} - \tfrac{18}{11} \mathbf{u}_{n-1} + \tfrac{9}{11} \mathbf{u}_{n-2} -
	\tfrac{2}{11} \mathbf{u}_{n-3} +  & =   \tfrac{6}{11}\Delta t  \mathbf{g}_n.
\end{align*}
}
%

Full space-time discretizations take the form of a block lower triangular matrix, $\mathcal{A}$, where each block corresponds to the full
spatial solution at one point in time. The Adams-Bashforth methods are explicit, so the block diagonal of $\mathcal{A}$ is the identity,
and off-diagonal blocks given by spatial discretization terms at previous time steps. Adams-Moulton have spatial discretization blocks
along the diagonal as well as off-diagonals. Finally, BDF methods only have the spatial discretization along the diagonal; off-diagonal
blocks consist only of the identity. Such a structure reduces the number of matrix nonzeros and connectivity compared with an
Adams-type scheme; however, the Adams-Moulton implicit methods gain an extra order of accuracy. For example, if we consider
two time-neighbors away, that is, approximate $\mathbf{u}_i$ using $\mathbf{u}_{i-1}$ and $\mathbf{u}_{i-2}$, BDF is $O(\Delta t^2)$
accurate, while Adams-Moulton is $O(\Delta t^3)$. 

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{Matrix structure}

\tcb{Must include mass matrix integration for using finite elements in space. This will be a diagonal scaling
on the left by $M^{-1}$. Definitely motivates using AIR because of invariance to row scaling. }


Are these matrices M-matrices?? May help motivate AMG.. If $\mathcal{L}$ is an M-matrix, BDF1/AM1 also is, but none of the others.
M-matrix must have non-positive off-diagonals.

If $\mathcal{L}$ is symmetric, $A^*A$ and $AA^*$ are almost identical, thus (hopefully) implying the left and right singular vectors are
similar. This would motivate a $P^*AP$ approach, which seems to be working well on diffusion. For BDF-type schemes, $A^*A$ and $AA^*$
seem to differ only in the first and last $k$ time steps, where $k$ is the order of the time integration.

Real symmetric matrix has SVD equivalent to eigendecomposition, with signs swapped on 




% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{BDF}

Suppose the spatial discretization at each time step has $n_{\mathbf{x}}$ degrees of freedom (over all dimensions) and the time domain
is discretized into $n_t$ time steps of size $\Delta t$. Let $N_p$ denote the number of processors.


%int HYPRE IJMatrixInitialize (HYPRE IJMatrix matrix)
%	Prepare a matrix object for setting coefficient values. 
%
%int HYPRE IJMatrixSetValues (HYPRE IJMatrix matrix, int nrows, int* ncols, const int* rows, const int* cols, const double* values)
%	Sets values for nrows rows or partial rows of the matrix. The arrays ncols and rows are of dimension nrows and contain the number of columns in each row and the row indices, respectively. The array cols contains the column indices for each of the rows, and is ordered by rows. The data in the values array corresponds directly to the column entries in cols. Erases any previous values at the specified locations and replaces them with new ones, or, if there was no value there before, inserts a new one.
%Not collective.
%
%HYPRE IJMatrixAssemble (HYPRE IJMatrix matrix)
%	Finalize the construction of the matrix before using
%

% ------------------------------------------------------------------------------------------------------- %
\subsubsection{Assume $N_p | n_t$}
% MPI ranks are zero-indexed. 

For an initial implementation, assume that $N_p < n_t$ and $N_p | n_t$ ($N_p$ divides $n_t$), where $P_T = \frac{n_t}{N_p}$. This is nice
because no time steps (and the corresponding matrix blocks) are split over multiple processors. That is, each processor will create its own
spatial discretizaion(s), and we do not need to split these over processors. Assume we are using zero-indexing. 

Structurally, the BDF discretizations are fairly straightforward. The right-hand side consists of $\mathbf{g}$ evaluated at the current time
and scaled by a constant. The diagonal block of the matrix consists of the identity plus some constant times the spatial discretization, and
there are $k$ off-diagonal blocks, each of which are a constant-diagonal matrix, where $k$ is the order of accuracy of the BDF discretization. 
Initial conditions, $\mathbf{u}_{-1} = f(\mathbf{x})$ are also easily satisfied by moving to the right-hand-side. Thus each block row of the matrix
takes on the CSR structure of the spatial discretization, with an extra $k$ nonzeros per row (except for points near the boundary).\\
\\
\noindent\textbf{Notes on construction:}
\begin{itemize}
\item For time $t_i$, the $i$th block row consists of rows $\{in_{\mathbf{x}}, ...,[(i+1)n_{\mathbf{x}}-1]\}$ in the matrix. 
\item Processor $p_{\ell}$ has time steps $\{P_T\ell,...,P_T(\ell+1)-1\}$. 
\item When adding spatial discretization matrices to the global structure, we will need to adjust the column indices. The $j$th block column
will have column indices $+jn_{\mathbf{x}}$. 
\item Suppose $\frac{n_t}{N_p} = q$. Then we will have $q$ time steps (i.e., block rows) stored on each processor and $qn_{\mathbf{x}}$
total rows. 
\item At time $t_0$ (first row in $\mathcal{A}$), there is only a diagonal block, consisting of the spatial discretization. Right hand side must
account for initial conditions. \tcb{How do we handle first time step(s) with high-order BDF scheme?}

\end{itemize}


% ------------------------------------------------------------------------------------------------------- %
\subsubsection{Assume $N_p > n_t$ and $n_t | N_p$}
% MPI ranks are zero-indexed. 

Now suppose there are more processors than time steps. For a large, 2d- or 3d-space problem, this is entirely possible.
Here we make a similar assumption as above, this time that $n_t | N_p$. Then, one time step will be split over multiple
processors. Let $N_p$ be the number of processors, $n_t$ the number of time steps, and $N_{p_x} = \frac{N_p}{n_t}$
be the number of processors per time index (i.e., number of processors per spatial discretization). For rank $r_i$, we
define communication groups 
\begin{align*}
\mathcal{G}_i = \left\{r_k \text{ $\Big|$ } \left\lfloor \frac{r_k}{N_{p_x}} \right\rfloor == i\right\}.
\end{align*}
These groups will be the communication packages that we build spatial discretizations over. 



% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{Boot-strapping}

For high-order integrators, there needs to be some sort of boot-strapping process such that the initial time step has sufficient
information to be high-order accuracy. One option is to take a very small time step, for example, use BDF1 to take a step of
size $\Delta t^2$, and proceed to use the initial condition and the solution at time $\Delta t^2$ to take a time step of size 
$\Delta t$. However, this introduces a variably-sized time steps, and high-order BDF methods can be unstable with variable
time steps. \todo{Talk about other options?}

The goal here is to design the space-time construction code to be such that the user modifies the right-hand-side appropriately
such that the first step(s) are of an appropriate order of accuracy. This should make the code fairly general / ``non-intrusive.'' 

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\section{Tensor-product structure?}


% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\newpage
\section{Time integration}

Using a method-of-lines approach, we typically formulate an initial value problem (IVP) as an ode in time,
with a general form along the lines of
\begin{align}
\mathbf{u}_t(t) = \mathbf{f}(\mathbf{u}, t) = \mathbf{k}(\mathbf{u},t) + \mathbf{g}(\mathbf{u},t), \label{eq:ode}
\end{align}
where $\mathbf{u}(t)$ is the full spatial solution at time $t$ and we break the right-hand side $\mathbf{f}$ into
temporally stiff terms, $\mathbf{k}$, and non-stiff terms, $\mathbf{g}$. Then, we rewrite \eqref{eq:ode} as
\begin{align}
\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) & = \mathcal{G}(\mathbf{u},t),\label{eq:imp_exp}
\end{align}
where $\mathcal{F}$ is the implicit part of our operator, typically chosen to correspond with the stiff part of \eqref{eq:ode},
which is integrated implicitly, and $\mathcal{G}$ is the explicit part of our operator, which is chosen to correspond with non-stiff
terms in \eqref{eq:ode}. Equation \eqref{eq:imp_exp} is a differential algebraic equation (DAE). The following table, taken from the
PetSc manual, gives a nice overview of how $\mathcal{F}$ and $\mathcal{G}$ are defined for various circumstances: 

%
\begin{table}[h!]
\small
\centering
\begin{tabular}{l l l} \hline
$\mathbf{u}_t = \mathbf{g}(\mathbf{u},t)$  & non-stiff ODE & $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) = \mathbf{u}_t$, 
	\\&& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = \mathbf{g}(\mathbf{u},t)$ \\
$\mathbf{u}_t = \mathbf{k}(\mathbf{u},t)$  & stiff ODE & $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) = \mathbf{u}_t - \mathbf{k}(\mathbf{u},t)$, 
	\\&& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = \mathbf{0}$ \\
$M\mathbf{u}_t = \mathbf{g}(\mathbf{u},t)$  & non-stiff ODE with & $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) = \mathbf{u}_t$, 
	\\& mass matrix& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = M^{-1}\mathbf{g}(\mathbf{u},t)$ \\
$M\mathbf{u}_t = \mathbf{k}(\mathbf{u},t)$  & stiff ODE with mass & $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) = M\mathbf{u}_t - \mathbf{k}(\mathbf{u},t)$, 
	\\&matrix & \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = \mathbf{0}$ \\\hline
$\mathbf{u}_t = \mathbf{k}(\mathbf{u},t) + \mathbf{g}(\mathbf{u},t)$  & stiff/non-stiff ODE &  $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) =
	\mathbf{u}_t - \mathbf{k}(\mathbf{u},t)$, \\&& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = \mathbf{g}(\mathbf{u},t)$ \\\hline
$M\mathbf{u}_t = \mathbf{k}(\mathbf{u},t) + \mathbf{g}(\mathbf{u},t)$  & stiff/non-stiff ODE &  $\mathcal{F}(\mathbf{u},\mathbf{u}_t,t) =
	M\mathbf{u}_t - \mathbf{k}(\mathbf{u},t)$, \\&with mass matrix& \hspace{3.5ex}$\mathcal{G}(\mathbf{u},t) = M^{-1}\mathbf{g}(\mathbf{u},t)$ \\\hline
\end{tabular}
\end{table}
%

Given the decomposition in \eqref{eq:imp_exp}, many time-stepping schemes can be formulated based on the action of
$\mathcal{G}$ and $\mathcal{F}^{-1}$.

\textbf{Example:} backward Euler\\
\indent Solve for $k = f(\mathbf{x} + \delta t\mathbf{k}, t + \delta t)$ \\
\indent $\mathbf{x} =  \mathbf{x} + \delta t \mathbf{k}$.

%Note, number of iterations with GMRES and no block scaling is, e.g., 225. Block-diagonal scaling reduces
%to 19, AIR reduces to 5. 
%
%
%%%%
%\begin{table}[h!]
%\centering
%{\footnotesize
%\begin{tabular}{| c r r r r r r c c c|} \hline
%order & np & DOFs & loc DOFs & nnz & OC & dt=dx & $\rho$ & it/solve & solve/step \\\hline
%2 & 4 & 110592 & 27648 & 4976640 & 1.41 & 0.0145 & 0.0041 & 4 & 1 \\
%2 & 16 & 442368 & 28224 & 19906560 & 1.39 & 0.0072 & 0.0039 & 4 & 1 \\
%2 & 64 & 1769472 & 27648 & 79626240 & 1.39 & 0.0036 & 0.0042 & 4 & 1 \\
%2 & 256 & 7077888 & 27648 & 318504960 & 1.38 & 0.0018 & 0.0041 & 4 & 1 \\
%3 & 4 & 196608 & 49152 & 15728640 & 1.46 & 0.0145 & 0.0063 & 4 & 2 \\
%3 & 16 & 786432 & 50176 & 62914560 & 1.42 & 0.0072& 0.0069 & 4 & 2 \\
%3 & 64 & 3145728 & 49152 & 251658240 & 1.41 & 0.0036 & 0.0068 & 4 & 2 \\
%3 & 256 & 12582912 & 49152 & 1006632960 & 1.39 & 0.0018 & 0.0067 & 4 & 2 \\
%4 & 4 & 307200 & 76800 & 38400000 & 1.61 & 0.0145 & 0.0111 & 5 & 3 \\
%4 & 16 & 1228800 & 78400 & 153600000 & 1.53 & 0.0072 & 0.0097 & 4 & 3 \\
%4 & 64 & 4915200 & 76800 & 614400000 & 1.48 & 0.0036 & 0.0121 & 5 & 3 \\
%4 & 256 & 19660800 & 76800 & -1837367296 & 1.46 & 0.0018 & 0.0098 & 4 & 3 \\\hline
%\end{tabular}
%}
%\caption{Results applying AIR to implicit time-stepping schemes with DG advection (MFEM ex. 9). 
%Here, we use A-stable SDIRK time integration schemes, and pick the order of time integration equal to
%the order of spatial discretization, and set $dt = dx$, to get the same accuracy in space and time. Solves
%are done without Krylov acceleration, with a relative residual tolerance of $10^{-8}$. Higher-order SDIRK
%schemes require solving multiple linear systems, as shown in the solves/step column. Convergence factors
%are $< 0.012$ in all iterations here. Convergence can degrade if we increase, for example, $dt \gg \sqrt{dx}$.
%In general such a scenario is not likely to come up; however, for parallel-in-time with MGRiT, this may come up
%if we were to coarsen in time and not in space, and we will have to test if spatial coarsening is necessary.}
%\end{table}



% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\newpage
\section{Block AIR in time domain}

Consider some linear multistep method as discussed in Section \ref{sec:disc}. Let us consider schemes
with at most four stages (e.g., BDF3 or AM4). The space-time matrix will take the form
%
\begin{align*}
A = 
\begin{pmatrix}
	A_{1} \\ 
	B_{1} & A_2 \\ 
	C_1 & B_2 & A_3 \\
	D_1 & C_2 & B_3 & A_4 \\
	& \ddots & \ddots & \ddots & \ddots \\
	& & D_{n-3} & C_{n-2} & B_{n-1} & A_n
\end{pmatrix},
\end{align*}
%
for some sparse matrices $A_i,B_i,C_d,D_i$. In such temporal discretizations, the sub-diagonal is
almost always the largest ``weight'' in a given row. Because of this, semi-coarsening in the time
dimension is a natural choice. Without loss of generality, suppose $n$ is even, and we choose the
first row as an F-point. Then, sub-matrices take the following form:
%
\begin{align*}
A_{ff} & = 
\begin{pmatrix}
	A_{1} \\ 
	C_1 & A_3 \\
	& C_3 & A_5 \\
	& & \ddots & \ddots  \\
	& & & C_{n-3} & A_{n-1}
\end{pmatrix},
\hspace{6ex}
A_{fc} = 
\begin{pmatrix}
	 \mathbf{0} & \\
	B_2 & \ddots\\
	D_2 & B_4  \\
	& D_4 & B_6  \\
	& & \ddots & \ddots & \ddots\\
	& & & D_{n-4} & B_{n-2} &  \mathbf{0}
\end{pmatrix},
\\
A_{cf} & = 
\begin{pmatrix}
	B_{1} \\ 
	D_1 & B_3 \\
	& D_3 & B_5 \\
	&& \ddots & \ddots \\
	&& & D_{n-3} & B_{n-1}
\end{pmatrix},
\hspace{4ex}
A_{cc} = 
\begin{pmatrix}
	A_{2} \\ 
	C_2 & A_4 \\
	& C_4 & A_6 \\
	& & \ddots & \ddots  \\
	& & & C_{n-2} & A_{n}
\end{pmatrix}.
\end{align*}
%
Note that we could also swap C-points and F-points. This would lead to a different framework, which will be
explored later. Let $A_{ff} = D_{ff} + L_{ff} = (I - (-L_{ff})D_{ff}^{-1})D_{ff}$. Then, a Neumann approximation 
can be used to approximate $A_{ff}^{-1}$ and an approximation to $R_{\textnormal{ideal}}$ given by
%
\begin{align*}
R = \begin{pmatrix} -A_{cf}\mathcal{D}_{ff}^{-1}(I - L_{ff}D_{ff}^{-1} + (L_{ff}D_{ff}^{-1})^2 + ...) & I\end{pmatrix},
\end{align*}
%
where $\mathcal{D}_{ff}$ is the diagonal block of $A_{ff}$. 
Let $Z_i$ denote the F-point block of $R$ for a degree-$i$ truncated Neumann approximation. Then
%
{\footnotesize
\begin{align*}
Z_0 & = -
\begin{pmatrix}
	B_{1}A_1^{-1} \\ 
	D_1A_1^{-1} & B_3A_3^{-1} \\
	& D_3A_3^{-1} & B_5A_5^{-1} \\
	&& \ddots & \ddots \\
	&& & D_{n-3}A_{n-3}^{-1} & B_{n-1}A_{n-1}^{-1}
\end{pmatrix}, \\
Z_1& = -
\begin{pmatrix}
	B_{1}A_1^{-1} \\ 
	D_1A_1^{-1} & B_3A_3^{-1} \\
	& D_3A_3^{-1} & B_5A_5^{-1} \\
	&& \ddots & \ddots \\
	&& & D_{n-3}A_{n-3}^{-1} & B_{n-1}A_{n-1}^{-1}
\end{pmatrix}
\begin{pmatrix}
	I \\ 
	-C_1A_1^{-1} & I \\
	& -C_3A_3^{-1} & I \\
	& & \ddots & \ddots  \\
	& & & -C_{n-3}A_{n-3}^{-1} & I
\end{pmatrix} \\
& = -
\begin{pmatrix}
	B_{1}A_1^{-1} \\ 
	D_1A_1^{-1} - B_3A_3^{-1}C_1A_1^{-1} & B_3A_3^{-1} \\
	-D_3A_3^{-1}C_1A_1^{-1} & D_3A_3^{-1} - B_5A_5^{-1}C_3A_3^{-1} & B_5A_5^{-1} \\
	& \ddots & \ddots & \ddots \\
%	& & D_{n-3}A_{n-3}^{-1}C_{n-5} & D_{n-3}A_{n-3}^{-1} + B_{n-1}A_{n-1}^{-1}C_{n-3} & B_{n-1}A_{n-1}^{-1}
\end{pmatrix}
\end{align*}
}%
For ease of notation and also likely important for a clean derivation, suppose we have a PDE with no
time-dependent coefficients. This means $A_1 = A_2 = ...$, $B_1 = B_2 = ...$, etc. \todo{What about
boundary conditions? Assuming we can move to rhs; need to verify.} Then
%
\begin{align*}
Z_0 & = -
\begin{pmatrix}
	BA^{-1} \\ 
	DA^{-1} & BA^{-1} \\
	& \ddots & \ddots \\
	& & DA^{-1} & BA^{-1}
\end{pmatrix}, \\
Z_1 & = -
\begin{pmatrix}
	BA^{-1} \\ 
	DA^{-1} - BA^{-1}CA^{-1} & BA^{-1} \\
	-DA^{-1}CA^{-1} & DA^{-1} - BA^{-1}CA^{-1} & BA^{-1} \\
	& \ddots & \ddots & \ddots \\
	& & -DA^{-1}CA^{-1} & DA^{-1} - BA^{-1}CA^{-1} & BA^{-1}
\end{pmatrix}
\end{align*}
%
Now, let us define $P$ as the famous one-point interpolation that has worked well for AIR, where we
interpolate each F-point from its strongest C-neighbor:
%
\begin{align*}
P = \begin{pmatrix} W \\ I \end{pmatrix} = 
\begin{pmatrix}
\mathbf{0} \\ 
I & \mathbf{0} \\
& \ddots & \ddots\\
& & I & \mathbf{0} \\ \hline
I \\
& \ddots \\
& & \ddots \\
& & & I
\end{pmatrix}
\end{align*}
%
Computing $\mathcal{K} := RAP$ in block form, we have $\mathcal{K} = RAP = ZA_{ff}W + A_{cf}W 
+ ZA_{fc}+A_{cc}$. Looking at each of these terms, we have
%
\begin{align*}
A_{cc} &= 
\begin{pmatrix}
	A \\ 
	C & A \\
	& \ddots & \ddots  \\
	& & C & A
\end{pmatrix},
\\
Z_0A_{fc} &=-
\begin{pmatrix}
	\mathbf{0} \\
	BA^{-1}B  & \mathbf{0} \\ 
	DA^{-1}B + BA^{-1}D & BA^{-1}B & \mathbf{0} \\
	& \ddots & \ddots & \ddots \\
	& & DA^{-1}B + BA^{-1}D & BA^{-1}B & \mathbf{0} 
\end{pmatrix},
\\
Z_1A_{fc} & =
\begin{pmatrix}
0
\end{pmatrix}, 
\\
A_{cf}W & = 
\begin{pmatrix}
	\mathbf{0} \\
	B  & \mathbf{0} \\ 
	D & B & \mathbf{0} \\
	& \ddots & \ddots & \ddots \\
	& & D & B & \mathbf{0} 
\end{pmatrix},
\hspace{4ex}
A_{ff}W = 
\begin{pmatrix}
	\mathbf{0} \\
	A & \mathbf{0} \\ 
	C & A & \mathbf{0}\\
	& \ddots & \ddots & \ddots  \\
	& & C & A & \mathbf{0}
\end{pmatrix},
\\
Z_0A_{ff}W & = -
\begin{pmatrix}
	\mathbf{0} \\
	B & \mathbf{0} \\ 
	D + BA^{-1}C & B & \mathbf{0} \\
	DA^{-1}C & D + BA^{-1}C & B & \mathbf{0} \\
	& \ddots & \ddots & \ddots & \ddots \\
	& & DA^{-1}C & D + BA^{-1}C & B & \mathbf{0}
\end{pmatrix},
\\
Z_1A_{ff}W & = -
\begin{pmatrix}
	\mathbf{0} \\
	B & \mathbf{0} \\ 
	D & B & \mathbf{0} \\
	-BA^{-1}CA^{-1}C & D & B & \mathbf{0} \\
	-DA^{-1}CA^{-1}C & -BA^{-1}CA^{-1}C & D & B & \mathbf{0} \\
	& \ddots & \ddots & \ddots & \ddots & \ddots \\
\end{pmatrix},
\end{align*}
%


Coarse-grid operators then take the form
%
\begin{align*}
\mathcal{K}_0 & =
\begin{pmatrix}
	A \\
	C - BA^{-1}B& A \\ 
	-BA^{-1}C & C- BA^{-1}B & A \\
	- DA^{-1}C & - BA^{-1}C & C - BA^{-1}B & A \\
	& \ddots & \ddots & \ddots & \ddots \\
	& & - DA^{-1}C & -BA^{-1}C & C - BA^{-1}B & A
\end{pmatrix},
\end{align*}
%
\tcb{$\mathcal{K}_0$ isn't quite finished.}





% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\newpage
\section{Runge-Kutta}

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{RK4:} 

Consider the classic RK4 time integration scheme as an example to solve $\mathbf{u}_t = \mathcal{L}\mathbf{u} + \mathbf{g}(t)$,
where $\mathcal{L}$ is a spatial discretization matrix and $\mathbf{u}_n$ the spatial solution in vector form at time $t_n$. 
Then, the four stages can be written out as:
%
\begin{align*}
k_1 & = \delta t\mathcal{L}\mathbf{u}_n + \delta t\mathbf{g}(t_n), \\
k_2 & = \delta t\mathcal{L}(\mathbf{u}_n + \tfrac{k_1}{2}) + \delta t\mathbf{g}(t_{n+\frac{1}{2}}) \\
	& = \delta t\mathcal{L}\mathbf{u}_n + \tfrac{\delta t^2}{2}\mathcal{L}^2\mathbf{u}_n + \delta t\mathbf{g}(t_{n+\frac{1}{2}}) +
	\tfrac{\delta t^2}{2}\mathcal{L}\mathbf{g}(t_n) ,\\
k_3 & = \delta t\mathcal{L}(\mathbf{u}_n + \tfrac{k_2}{2}) + \delta t\mathbf{g}(t_{n+\frac{1}{2}}) \\
	& = \delta t\mathcal{L}\mathbf{u}_n + \tfrac{\delta t^2}{2}\mathcal{L}^2\mathbf{u}_n + \tfrac{\delta t^3}{4}\mathcal{L}^3\mathbf{u}_n +
	\delta t(I+\tfrac{\delta t}{2}\mathcal{L})\mathbf{g}(t_{n+\frac{1}{2}}) + \tfrac{\delta t^3}{4}\mathcal{L}^2\mathbf{g}(t_n), \\
k_4 & = \delta t\mathcal{L}(\mathbf{u}_n + k_3) + \delta t \mathbf{g}(t_{n+1}), \\
	& = \delta t\mathcal{L}\mathbf{u}_n + \delta t^2\mathcal{L}^2\mathbf{u}_n + \tfrac{\delta t^3}{2}\mathcal{L}^3\mathbf{u}_n +
	\tfrac{\delta t^4}{4}\mathcal{L}^4\mathbf{u}_n + \delta t^2\mathcal{L}(I+\tfrac{\delta t}{2}\mathcal{L})\mathbf{g}(t_{n+\frac{1}{2}}) +
	 ...\\&\hspace{4ex} \tfrac{\delta t^4}{4}\mathcal{L}^3\mathbf{g}(t_n) + \delta t \mathbf{g}(t_{n+1}),
\end{align*}
%
where 
%
\begin{align*}
\mathbf{u}_{n+1} & = \mathbf{u}_n + \tfrac{1}{6}(k_1+2k_2+2k_3+k_4).
\end{align*}
%
These stages can expressed algebraically as a space-time matrix. For RK4, such a matrix takes the block form:
%
{\footnotesize
\begin{align}
\begin{split}
%\begin{pmatrix} I &  \\ -\mathcal{S} & I \end{pmatrix}\hspace{3ex}\sim\hspace{3ex}
\begin{pmatrix}
\ddots \\ 
-\tfrac{1}{6}I &I &  &  &  &   & \\
&-\delta t \mathcal{L} & I &  &  &  &  \\
&-\delta t \mathcal{L} & -\tfrac{\delta t}{2} \mathcal{L} & I &   &  & \\
&-\delta t \mathcal{L} &  & -\tfrac{\delta t}{2} \mathcal{L} & I  &  & \\
&-\delta t \mathcal{L} &  &  & -\delta t \mathcal{L} & I &  \\
&-I & -\frac{1}{6}I & -\frac{1}{3}I & -\frac{1}{3}I & -\frac{1}{6}I & I \\
&&&&&& -\delta t \mathcal{L} & \ddots
\end{pmatrix}
\begin{pmatrix} \vdots \\ \mathbf{u}_n \\ k_1^{(n)} \\ k_2^{(n)} \\ k_3^{(n)} \\ k_4^{(n)} \\ \mathbf{u}_{n+1}  \\ \vdots \end{pmatrix}
= \\
\begin{pmatrix} \vdots \\ \mathbf{0} \\ \delta t\mathbf{g}(t_n) \\  \delta t\mathbf{g}(t_{n+\frac{1}{2}}) +
	\tfrac{\delta t^2}{2}\mathcal{L}\mathbf{g}(t_n) \\ \delta t(I+\tfrac{\delta t}{2}\mathcal{L})\mathbf{g}(t_{n+\frac{1}{2}}) +
	\tfrac{\delta t^3}{4}\mathcal{L}^2\mathbf{g}(t_n) \\ \delta t^2\mathcal{L}(I+\tfrac{\delta t}{2}\mathcal{L})\mathbf{g}(t_{n+\frac{1}{2}}) +
	\tfrac{\delta t^4}{4}\mathcal{L}^3\mathbf{g}(t_n) + \delta t \mathbf{g}(t_{n+1}) \\ \mathbf{0}  \\ \vdots
\end{pmatrix}
\end{split}\label{eq:s_stage}
\end{align}
%
}Going from the expanded $s$-stage form to a condensed $2\times 2$ block matrix can be achieved by straightforward
elimination of variables $k_1,k_2,...$. Combining, we have 
%
\begin{align*}
\mathbf{u}_{n+1} & = \mathbf{u}_n + \tfrac{1}{6}(k_1+2k_2+2k_3+k_4) \\
	& = \mathcal{S}\mathbf{u}_n + \mathcal{G}_n, \\
\mathcal{S} & = I + \delta t\mathcal{L} + \tfrac{\delta t^2}{2}\mathcal{L}^2 + \tfrac{\delta t^3}{6}\mathcal{L}^3 +
	\tfrac{\delta t^4}{24}\mathcal{L}^4, \\
\mathcal{G}_n & = \tfrac{\delta t}{6}\Big[ I  +\delta t \mathcal{L} + \tfrac{\delta t^2}{2}\mathcal{L}^2 + \tfrac{\delta t^3}{4}\mathcal{L}^3\Big] \mathbf{g}(t_{n}) +
	\tfrac{\delta t}{3}\Big[2I + \delta t \mathcal{L} + \tfrac{\delta t^2}{4}\mathcal{L}^2\Big]\mathbf{g}(t_{n+\frac{1}{2}}) + \tfrac{\delta t}{6} \mathbf{g}(t_{n+1}).
\end{align*}
%
In matrix form, this looks like
%
\begin{align} \label{eq:condensed}
\begin{pmatrix} 
I \\ -\mathcal{S} & I \\ & -\mathcal{S} & I \\ && \ddots & \ddots \end{pmatrix} \begin{pmatrix} \mathbf{u}_0 \\ \mathbf{u}_1 \\\mathbf{u}_2 \\ \vdots \end{pmatrix}
	= \begin{pmatrix} \mathcal{G}_0 \\ \mathcal{G}_1 \\ \mathcal{G}_2 \\ \vdots \end{pmatrix}.
\end{align}
%
Note that all explicit RK methods will take such a form, where $\mathcal{S}$ consists of some polynomial in $\mathcal{L}$.
\tcb{This right-hand side seems quite complicated to this. Does MGRiT arrive at same structure? I suppose this is in the user's hands...}

By the single sub-diagonal structure here, we can actually form $R_{ideal}$. In particular, operators take the form
%
\begin{align*}
R_{ideal} & = \begin{pmatrix}-\mathcal{S}^2 & & & I & \\ & -\mathcal{S}^2 & & & I \\ & & \ddots & & & \ddots\end{pmatrix} \\
R_{ideal}AP & = \begin{pmatrix} I \\ -\mathcal{S}^2 & I \\ & -\mathcal{S}^2 & I \\ & & \ddots & \ddots \end{pmatrix}.
\end{align*}
%
Note that this holds regardless of interpolation to F-points. In fact, the restriction only to C-points as used in MGRiT actually makes
sense if you form $R_{ideal}$ because that is the $\ell^2$-orthogonal projection. \tcb{Has MGRiT been tried with $R_{ideal}$ instead
of $P_{ideal}$? In all AIR tests $R_{ideal}$ has performed better; it is also setting the error equal to zero at C-points, which is typically preferable
to the residual, which is achieved by $P_{ideal}$..} In any case, notice this is exactly as arrived at in
MGRiT, albeit in explicit algebraic form: the coarse-grid operator consists of taking two steps, $\mathcal{S}^2$, using the fine-grid
time stepper. Denote $\mathcal{S}_{\delta t}$ as $\mathcal{S}$ with time step $\delta t$. Expanding, 
%
\begin{align*}
\mathcal{S}_{\delta t}^2 & = \Big( I + \delta t\mathcal{L} + \tfrac{\delta t^2}{2}\mathcal{L}^2 + \tfrac{\delta t^3}{6}\mathcal{L}^3 +
	\tfrac{\delta t^4}{24}\mathcal{L}^4\Big)^2 \\
& = I + 2 \delta t \mathcal{L} + 2 \delta t^2\mathcal{L}^2 +  \tfrac{4\delta t^3}{3}\mathcal{L}^3 + \tfrac{2\delta t^4}{3}\mathcal{L}^4 + \tfrac{\delta t^5}{4}\mathcal{L}^5 + \tfrac{5\delta t^6}{72}\mathcal{L}^6 + \tfrac{\delta t^7}{72}\mathcal{L}^7 + \tfrac{\delta t^8}{576} \mathcal{L}^8 \\
& = \mathcal{S}_{2\delta t} + O((\delta t\mathcal{L})^5).
\end{align*}
%
That is, taking a time step of size $2\delta t$, as done in MGRiT, is, in the Taylor-sense, the best approximation to two steps on 
the fine grid. However, this assumes that $\|\delta t\mathcal{L}\| < 1$. \tcb{Is this more or less enforced by the stability criterion
of explicit time integrators? Also, is this unique to RK4, or for explicit RK is it generally the case that taking a time step of $2\delta t$
is, in a Taylor-sense, the best approximation to two time steps?}

% ------------------------------------------------------------------------------------------------------- %
% ------------------------------------------------------------------------------------------------------- %
\subsection{General RK schemes}

Returning to \eqref{eq:s_stage}, one can notice that the block matrix resembles that of the Butcher Tableaux for RK4. 
Implicit schemes generally cannot be reduced to a condensed block form, as in \eqref{eq:condensed}. However, given the Butcher
Tablueax for an $s$-stage Runge-Kutta scheme, implicit or explicit, the matrix will take on a block structure as in \eqref{eq:s_stage}
with the following blocks (overlapping other blocks in the first and last row/column):
%
\begin{align} \label{eq:gen_block}
\begin{pmatrix} 
I  & \mathbf{0} & \mathbf{0} & ... & \mathbf{0} & \mathbf{0}\\
-\delta t\mathcal{L} & I - a_{11}\delta t\mathcal{L} & -a_{12}\delta t\mathcal{L} & ... &  -a_{1s}\delta t\mathcal{L} & \mathbf{0} \\
-\delta t\mathcal{L} & -a_{21}\delta t\mathcal{L} & I -a_{22}\delta t\mathcal{L} & ... & -a_{2s}\delta t\mathcal{L} & \mathbf{0} \\ 
\vdots  & \vdots & \vdots & \ddots & \vdots  & \vdots \\
-\delta t\mathcal{L} & -a_{s1}\delta t\mathcal{L} & -a_{s2}\delta t\mathcal{L} & ...  & I - a_{ss}\delta t\mathcal{L} & \mathbf{0} \\ 
-I & -b_1I & -b_2I & ... & -b_s I & I
\end{pmatrix}
\begin{pmatrix} \mathbf{u}_n \\ \mathbf{k}_1^{(n)} \\ \mathbf{k}_2^{(n)} \\ \vdots \\ \mathbf{k}_s^{(n)} \\ \mathbf{u}_{n+1} \end{pmatrix} =
\end{align}
%
For diagonally implicit Runge-Kutta (DIRK) methods, the blocks in \eqref{eq:gen_block} are lower triangular. For explicit
methods, they are lower triangular with unit diagonal. This allows for a natural elimination of middle stages for the condensed
form in \eqref{eq:condensed}. \tcb{Would such a matrix be algebraically amenable to AIR-type algorithms? Unfortunately, the
number of rows in the matrix scales with $s$...} We can also condense \eqref{eq:gen_block} using Kronecker products for a
block matrix structure:
%
\begin{align*}
\begin{pmatrix}
I_{n\times n} &  \\
-\delta t(\mathbf{1}_n \otimes \mathcal{L}) & I_{ns\times ns} - \delta t A\otimes \mathcal{L} &\\
-I_{n\times n} & -\mathbf{b}\otimes I_{n\times n} & I_{n\times n} \\ 
& & -\delta t(\mathbf{1}_n \otimes \mathcal{L}) & I_{ns\times ns} - \delta t A\otimes \mathcal{L} &  \\
& & -I_{n\times n} & -\mathbf{b}\otimes I_{n\times n} & I_{n\times n} \\
&&& \ddots\text{\hspace{4ex}}  & \ddots\text{\hspace{8ex}}  & \ddots
\end{pmatrix}.
\end{align*}
%
In the spirit of algebraic reduction, suppose we designate solution blocks (with identity on diagonal) as C-points and
the $s$-stage blocks as F-points. Then,
%
{\footnotesize
\begin{align*}
-A_{cf}A_{ff}^{-1} & = \begin{pmatrix} \mathbf{0} \\ -(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1} \\
&  -(\mathbf{b}\otimes I) (I - \delta tA\otimes \mathcal{L})^{-1} \\ & & \ddots \end{pmatrix} , \\
R_{ideal}AP & =  \begin{pmatrix} I \\ -I -\delta t(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L})  & I \\
& -I - \delta t(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L})  & I \\ & & \ddots & \ddots \end{pmatrix}.
\end{align*}
%
}Notice that this now takes on a form that we can apply a more traditional reduction to with an identity on the diagonal. $A_{ff}^{-1}$ is
block diagonal and invertible. In the case of DIRK methods, $ (I - \delta tA\otimes \mathcal{L})^{-1} $ can be inverted through a block
forward solve, almost like F-relaxation through Runge-Kutta stages. For non-DIRK methods, solving should still be feasible, but is more
complicated. \footnote{
We could also swap the choice of C- and F-points. Then, $A_{ff}^{-1}$ can be directly computed, but is a global, dense lower triangular
matrix coupling all DOFs.} In any case, we are only interested in the solution at specific points in time, corresponding to C-points, and
do not need the intermediate RK stages. Because we can exactly compute $R_{ideal}$, which gives an exact solution at C-points, all we
need now is to solve the coarse-grid problem. Thus, we have projected the problem to dimension independent of $s$ (kind of, although
the diagonal of $A_{ff}$ interpolates to a large problem, inverts there, and brings it back; this particularly motivates DIRK methods). 
One idea is to apply a reduction-based multigrid, exactly as we can do with explicit RK methods.
\tcb{What's interesting is the $\delta t$ inside of $(I - \delta tA\otimes\mathcal{L})^{-1}$.
When we get a coarse-grid operator with an $\mathcal{S}^2$-like term on the subdiagonal, expanding the subdiagonal:
%
\begin{align*}
&\Big[ -I - \delta t(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L}) \Big]^2 =\\
	 & \hspace{4ex}I + 2\delta t (\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L}) + \delta t^2\Big[(\mathbf{b}\otimes I )(I - \delta tA\otimes \mathcal{L})^{-1}(\mathbf{1}_n \otimes \mathcal{L}) \Big]^2.
\end{align*}
%
Is this a flaw in the derivation approach? Is it better to do a coarse-grid time step with $2\delta t$ everywhere, or to actually use
$\delta t$ to advance the $s$ stages and $2\delta t$ for the time step??
}






\end{document}







